{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610082d9-e22c-4113-b7cd-47fff322a40e",
   "metadata": {},
   "source": "## Step 1: Create OSS Collection"
  },
  {
   "cell_type": "markdown",
   "id": "c314d204-1cc2-41f7-afdf-09e3aa854359",
   "metadata": {},
   "source": [
    "# Image Vector Storage and Search Pipeline with OpenSearch\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to the second notebook in our image processing series. This notebook builds upon the image processing completed in the previous notebook and focuses on creating a sophisticated vector search system using Amazon OpenSearch Serverless. We'll transform our processed images into searchable vector embeddings, enabling powerful semantic search capabilities that go beyond traditional keyword matching.\n",
    "\n",
    "Vector search represents a significant advancement in image retrieval systems. Unlike conventional methods that rely on tags or metadata, vector search converts images into high-dimensional vector representations (embeddings) that capture both visual and semantic information. This approach enables us to find similar images based on their actual content and meaning, rather than just matching keywords.\n",
    "\n",
    "The implementation leverages Amazon's Titan Multimodal Embeddings model, a state-of-the-art AI system capable of understanding both images and text. By generating vector representations of our images, we create a sophisticated search system that can understand and match images based on natural language queries. This is particularly valuable for applications requiring intuitive image retrieval, content recommendation, or visual similarity matching.\n",
    "\n",
    "\n",
    "This notebook utilizes several AWS services, including:\n",
    "\n",
    "* **Amazon OpenSearch Serverless** for vector storage and search\n",
    "* **Amazon Bedrock** with Titan Multimodal for embedding generation\n",
    "* **Amazon S3** for data storage\n",
    "* **Amazon SageMaker** for notebook hosting\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* **Vector Database Management**:\n",
    "  * Index creation and configuration\n",
    "  * Bulk data ingestion\n",
    "  * Efficient vector storage\n",
    "\n",
    "* **Embedding Generation**:\n",
    "  * Multimodal embedding creation\n",
    "  * Dimension optimization\n",
    "  * Batch processing support\n",
    "\n",
    "* **Semantic Search Capabilities**:\n",
    "  * k-NN search implementation\n",
    "  * Query vector generation\n",
    "  * Result visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd7fad-2585-434b-9338-4dd54f9247cb",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup and Dependencies](#Setup-and-Dependencies)\n",
    "2. [Configuration and Environment Setup](#Configuration-and-Environment-Setup)\n",
    "3. [Embedding Generation](#Embedding-Generation)\n",
    "4. [OpenSearch Client Creation](#OpenSearch-Client-Creation)\n",
    "5. [Index Creation and Management](#Index-Creation-and-Management)\n",
    "6. [Data Ingestion](#Data-Ingestion)\n",
    "7. [Search Implementation](#Search-Implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503b464-ec01-4fa2-87d2-ede4718ca903",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Ensure that you are using the python kernel <b>conda_python3</b>\n",
    "</div>\n",
    "\n",
    "## Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f711b77-e506-4665-af48-f457b8cac497",
   "metadata": {
    "tags": []
   },
   "source": [
    "import random\n",
    "import os\n",
    "import nbimporter\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "from IPython.display import display, Image as IPImage, Markdown\n",
    "import nbimporter\n",
    "import boto3\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from _00_image_processing import resize_and_encode\n",
    "import time\n",
    "from typing import Any, Optional, List, Dict\n",
    "\n",
    "%store -r REGION\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", REGION)\n",
    "aoss_client = boto3.client(\"opensearchserverless\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3bdde23-b66a-4d47-b89f-f8faa506f81a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16785bb4-8064-46df-994a-729d2899a1a6",
   "metadata": {},
   "source": [
    "## Configuration and Environment Setup\n",
    "Loading stored variables from previous notebook and initializing clients."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6197aad-312f-409b-b002-37e8b16f3bce",
   "metadata": {
    "tags": []
   },
   "source": [
    "%store -r VECTOR_STORE_NAME_PREFIX REGION image_paths BASE64_IMAGES_DIR IMAGES_DIR image_descriptions BUCKET INDEX_NAME COLLECTION_ID  COLLECTION_ENDPOINT"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "615a74ce-ddad-440e-b338-0f65897e468e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Variables are imported from the previous notebook using IPython's %store magic command. Make sure you've run the first notebook successfully before proceeding.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736d8ef-95bb-42bb-8898-d67fb065a205",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c27ec-c7ce-4f86-8f08-f56bfeaaae61",
   "metadata": {},
   "source": [
    "## Embedding Generation\n",
    "\n",
    "The embedding generation process is a crucial step that transforms our images into mathematical representations suitable for vector search. We utilize Amazon's Titan Multimodal model, which excels at understanding both visual and textual content.\n",
    "\n",
    "The `get_titan_multimodal_embedding` function serves as our primary tool for generating these embeddings. It can process both images and text descriptions, making it versatile for our needs. The function:\n",
    "\n",
    "1. Accepts either an image path or text description as input\n",
    "2. Handles the base64 encoding of images when necessary\n",
    "3. Configures the embedding dimension (default 1024) for optimal performance\n",
    "4. Manages the API communication with Amazon Bedrock\n",
    "5. Returns normalized vector embeddings ready for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "61ac1675",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_titan_multimodal_embedding(\n",
    "    bedrock_runtime: Any,\n",
    "    image_path: Optional[str] = None,\n",
    "    description: Optional[str] = None,\n",
    "    dimension: int = 1024\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Generates a multimodal embedding using Amazon Titan for either an image or a text description.\n",
    "\n",
    "    Args:\n",
    "        bedrock_runtime: Bedrock client instance\n",
    "        image_path: Path to image file\n",
    "        description: Text description\n",
    "        dimension: Desired embedding dimension\n",
    "\n",
    "    Returns:\n",
    "        Dict containing embedding vector\n",
    "    \"\"\"\n",
    "    payload_body: Dict[str, Any] = {}\n",
    "\n",
    "    if image_path:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            payload_body[\"inputImage\"] = resize_and_encode(image_path)\n",
    "    if description:\n",
    "        payload_body[\"inputText\"] = description\n",
    "\n",
    "    embedding_config: Dict[str, Dict[str, int]] = {\n",
    "        \"embeddingConfig\": {\"outputEmbeddingLength\": dimension}\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps({**payload_body, **embedding_config}),\n",
    "        modelId=\"amazon.titan-embed-image-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    return json.loads(response.get(\"body\").read())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87ca2644-52e5-4692-b00e-59e90cda9a8d",
   "metadata": {},
   "source": [
    "Generate embeddings for all images"
   ]
  },
  {
   "cell_type": "code",
   "id": "91658e5f-56cc-4422-a1ae-911649b1c077",
   "metadata": {
    "tags": []
   },
   "source": [
    "embeddings = [\n",
    "    get_titan_multimodal_embedding(\n",
    "        bedrock_runtime=bedrock_runtime,\n",
    "        image_path=path,\n",
    "        dimension=1024\n",
    "    )\n",
    "    for path in image_paths\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bab21ef-4475-4a5a-ba27-293df550c343",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Processing:</b> Generating embeddings for all images. This process may take several minutes depending on the number and size of images, as each image needs to be processed by the Titan model.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e0262-2f6d-4cd3-b421-8ddd120bf253",
   "metadata": {},
   "source": [
    "Slice of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "17c7f013-3fe6-44ad-a2a1-1a908c0e0668",
   "metadata": {
    "tags": []
   },
   "source": [
    "embeddings[0]['embedding'][:10]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f95bf195-2895-485d-bf19-154e2c0d7b21",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189785f-8460-494d-978f-080fbef240d1",
   "metadata": {},
   "source": [
    "## OpenSearch Client Creation\n",
    "\n",
    "The establishment of a proper connection to OpenSearch represents a critical infrastructure component of our vector search system. This section handles the authentication and security requirements necessary for deployment.\n",
    "\n",
    "Our `get_oss_client` function implements several essential security features:\n",
    "1. **AWS IAM Authentication**: Utilizes AWS4SignerAuth for secure identity verification\n",
    "2. **SSL/TLS Configuration**: Establishes encrypted connections to protect data in transit\n",
    "3. **Connection Pooling**: Manages persistent connections for optimal performance\n",
    "4. **Error Handling**: Implements robust error catching and reporting\n",
    "\n",
    "This robust setup ensures our vector search system maintains security best practices while providing reliable performance for production workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "27a0d041-de53-49f9-9c02-f0d4689db83f",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_oss_client(\n",
    "    collection_endpoint: str,\n",
    "    region: str\n",
    ") -> OpenSearch:\n",
    "    \"\"\"\n",
    "    Creates an OpenSearch client with AWS authentication.\n",
    "\n",
    "    Args:\n",
    "        collection_endpoint: OpenSearch endpoint\n",
    "        region: AWS region\n",
    "\n",
    "    Returns:\n",
    "        Configured OpenSearch client\n",
    "    \"\"\"\n",
    "    return OpenSearch(\n",
    "        hosts=[{'host': collection_endpoint, 'port': 443}],\n",
    "        http_auth=AWSV4SignerAuth(boto3.Session().get_credentials(), region, 'aoss'),\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        use_ssl_context=True,\n",
    "        ssl_assert_hostname=False,\n",
    "        ssl_show_warn=False,\n",
    "        timeout=60,\n",
    "        max_retries=3,\n",
    "        retry_on_timeout=True\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6f8e81e-c553-4fc0-ac44-21d0be794775",
   "metadata": {
    "tags": []
   },
   "source": [
    "oss_client = get_oss_client(COLLECTION_ENDPOINT, region=REGION)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae3c49e3-9de6-4491-bb0d-cafc4429ff2a",
   "metadata": {},
   "source": [
    "## Index Creation and Management\n",
    "\n",
    "The index creation phase establishes the foundation for our vector search capabilities. We configure a specialized OpenSearch index optimized for k-NN (k-Nearest Neighbors) vector search operations. The index configuration involves several crucial components:\n",
    "\n",
    "### Index Structure\n",
    "The index mapping defines four key fields:\n",
    "- **image_vector**: A high-dimensional vector field (1024D) storing our image embeddings\n",
    "- **description**: Text field containing image descriptions\n",
    "- **image_base64_s3_uri**: Reference to the encoded image in S3\n",
    "- **image_s3_uri**: Original image location in S3"
   ]
  },
  {
   "cell_type": "code",
   "id": "312536f3-f462-4100-8400-310dd9af229d",
   "metadata": {
    "tags": []
   },
   "source": [
    "index_body: Dict[str, Any] = {\n",
    "    \"settings\": {\"index.knn\": \"true\"},\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"image_vector\": {\"type\": \"knn_vector\", \"dimension\": 1024},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"image_base64_s3_uri\": {\"type\": \"text\"},\n",
    "            \"image_s3_uri\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "oss_client.indices.create(index=INDEX_NAME, body=index_body)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9c41b90a-e579-43fa-8ad8-5c8347d2a47f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c59263-2b1d-413e-a120-1d81c6c250b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Ingestion\n",
    "\n",
    "The data ingestion process represents a critical phase where we populate our OpenSearch index with vector embeddings and associated metadata. This section implements a sophisticated ETL (Extract, Transform, Load) pipeline that handles:\n",
    "\n",
    "### Data Processing Steps:\n",
    "1. **Extraction**: \n",
    "   - Retrieves processed images from designated directories\n",
    "   - Sorts files to maintain consistent ordering\n",
    "   - Validates file extensions and formats\n",
    "\n",
    "2. **Transformation**:\n",
    "   - Pairs embeddings with corresponding metadata\n",
    "   - Structures data according to our index mapping\n",
    "   - Generates S3 URIs for both original and encoded images\n",
    "\n",
    "3. **Loading**:\n",
    "   - Implements batch processing with progress tracking\n",
    "   - Includes error handling and retry logic\n",
    "   - Maintains rate limiting to prevent service throttling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab88ed-0ff3-4a23-8d29-164fad9039ae",
   "metadata": {},
   "source": [
    "Extract the paths of the text files and images"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5d6c2d1-3c31-4f10-9845-5d1c4c68b86f",
   "metadata": {},
   "source": [
    "_base64_images = sorted([f for f in os.listdir(BASE64_IMAGES_DIR) if f.endswith(\".txt\")])\n",
    "_images = sorted([f for f in os.listdir(IMAGES_DIR) if f.endswith((\".jpg\", \".png\"))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3bf3574-5972-441f-b6f8-aaa0fd72aae1",
   "metadata": {},
   "source": [
    "Iterate through embeddings, descriptions, and image metadata to index in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "id": "641b431b-5307-4b17-95c0-7125bef84439",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(\"Ingesting data into OpenSearch...\")\n",
    "for embedding, description, base64_img, img_path in zip(\n",
    "    embeddings, image_descriptions, _base64_images, _images\n",
    "):\n",
    "    document: Dict[str, Any] = {\n",
    "        \"image_vector\": embedding['embedding'],\n",
    "        \"description\": description,\n",
    "        \"image_base64_s3_uri\": f\"s3://{BUCKET}/{BASE64_IMAGES_DIR}/{base64_img}\",\n",
    "        \"image_s3_uri\": f\"s3://{BUCKET}/{IMAGES_DIR}/{img_path}\"\n",
    "    }\n",
    "    oss_client.index(index=INDEX_NAME, body=document)\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"Data ingestion complete.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "572985ff-ab54-4458-a200-0fda96f706dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Processing:</b> The ingestion process includes a 5-second delay between documents to prevent rate limiting. This process may take a couple of minutes depending on the number of images. Retry if you encounter an error.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33b028-1dce-481b-a3f3-deee02cb13d7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482a470-61fc-453f-91fa-d3a5bb7e09e3",
   "metadata": {},
   "source": [
    "## Search Implementation\n",
    "\n",
    "The search implementation represents the culmination of our vector search pipeline, enabling sophisticated similarity-based image retrieval. Our system implements a k-NN (k-Nearest Neighbors) search strategy that leverages the vector space to find visually and semantically similar images.\n",
    "\n",
    "### Search Architecture\n",
    "The `query_open_search` function implements a multi-stage process:\n",
    "\n",
    "1. **Query Processing**:\n",
    "   - Converts natural language queries into vector embeddings\n",
    "   - Configures search parameters including result count\n",
    "   - Optimizes query structure for performance\n",
    "\n",
    "2. **Vector Similarity Search**:\n",
    "   - Executes k-NN algorithm over the vector space\n",
    "   - Calculates similarity scores between query and stored vectors\n",
    "   - Ranks results based on vector distance metrics\n",
    "\n",
    "3. **Result Management**:\n",
    "   - Filters and formats search results\n",
    "   - Excludes unnecessary vector data from responses\n",
    "   - Provides relevant metadata for result presentation\n",
    "\n",
    "This implementation enables natural and intuitive image search capabilities, allowing users to find relevant images using plain language descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a12159c1-6ea8-4377-ad5b-93c710ba141d",
   "metadata": {
    "tags": []
   },
   "source": [
    "def query_open_search(\n",
    "    bedrock_runtime: Any,\n",
    "    oss_client: OpenSearch,\n",
    "    index_name: str,\n",
    "    prompt: str,\n",
    "    top_k: int = 3\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Queries the OpenSearch index using a k-NN search with the given text prompt.\n",
    "\n",
    "    Args:\n",
    "        bedrock_runtime: Bedrock client instance\n",
    "        oss_client: OpenSearch client instance\n",
    "        index_name: Name of the OpenSearch index\n",
    "        prompt: Text query for searching\n",
    "        top_k: Number of top results to retrieve\n",
    "\n",
    "    Returns:\n",
    "        List of search results from OpenSearch\n",
    "    \"\"\"\n",
    "    query_emb: List[float] = get_titan_multimodal_embedding(\n",
    "        bedrock_runtime=bedrock_runtime,\n",
    "        description=prompt,\n",
    "        dimension=1024\n",
    "    )[\"embedding\"]\n",
    "\n",
    "    query_body: Dict[str, Any] = {\n",
    "        \"size\": top_k,\n",
    "        \"_source\": {\n",
    "            \"exclude\": [\"image_vector\"],\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"image_vector\": {\n",
    "                    \"vector\": query_emb,\n",
    "                    \"k\": top_k\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = oss_client.search(index=index_name, body=query_body)\n",
    "    return response[\"hits\"][\"hits\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf8329fb-0b6b-4971-90eb-187ddbec20cc",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ebdf4-b4f6-43cf-845e-1a90f3f4b91b",
   "metadata": {},
   "source": [
    "### Example Usage and Visualization\n",
    "\n",
    "This section demonstrates the practical application of our vector search system through a concrete example. We implement a visual search interface that:\n",
    "\n",
    "1. **Query Processing**:\n",
    "   - Accepts a natural language search prompt (\"building\" in this example)\n",
    "   - Converts the query into a vector embedding\n",
    "   - Executes the search against our index\n",
    "\n",
    "2. **Result Handling**:\n",
    "   - Downloads matched images from S3\n",
    "   - Formats descriptions for display\n",
    "   - Generates a clean, visual presentation\n",
    "\n",
    "3. **Display Formatting**:\n",
    "   - Implements consistent image sizing\n",
    "   - Creates a structured markdown layout\n",
    "   - Pairs images with their descriptions\n",
    "\n",
    "The visualization provides a user-friendly way to validate search results and demonstrate the system's effectiveness in finding relevant images based on textual queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf1e4030-2bc9-410e-8d11-dca560f7d30b",
   "metadata": {},
   "source": [
    "search_prompt = \"building\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d36726a-706d-452e-b558-0d22b76cf15e",
   "metadata": {},
   "source": [
    "Query index and display results"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f1a10ad-8876-4d8c-bd1e-b3ec93b65723",
   "metadata": {
    "tags": []
   },
   "source": [
    "markdown_content = \"\"\n",
    "IMAGE_WIDTH = 500\n",
    "\n",
    "try:\n",
    "    results = query_open_search(\n",
    "        bedrock_runtime=bedrock_runtime,\n",
    "        oss_client=oss_client,\n",
    "        index_name=INDEX_NAME,\n",
    "        prompt=search_prompt,\n",
    "        top_k=1\n",
    "    )\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        description = result[\"_source\"][\"description\"]\n",
    "        description = description.replace('\\n', '')\n",
    "        image_uri = result[\"_source\"][\"image_s3_uri\"]\n",
    "        _desc = f\"**Result {idx + 1}**: \\n{description}\"\n",
    "\n",
    "        local_image_path = f\"./image_download/result_{idx + 1}.jpg\"\n",
    "        !aws s3 cp \"$image_uri\" \"$local_image_path\"\n",
    "\n",
    "        markdown_content += f\"\"\"\n",
    "| <img src=\"{local_image_path}\" width=\"{IMAGE_WIDTH}\"/> |\n",
    "|:----------------:|\n",
    "| {_desc} |\n",
    "\n",
    "---\n",
    "        \"\"\"\n",
    "    display(Markdown(markdown_content))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "242d47ec-35a7-4d1a-8dec-211dc9d5526a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad80d7-1403-4b72-b15c-87eddde86e24",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ðŸŽ‰ Congratulations!</b> You have successfully completed the vector storage and search notebook!\n",
    "\n",
    "Key accomplishments:\n",
    "- âœ… Generated image embeddings using Titan Multimodal\n",
    "- âœ… Created and configured OpenSearch index\n",
    "- âœ… Ingested vector data\n",
    "- âœ… Implemented semantic search functionality\n",
    "- âœ… Demonstrated search capabilities\n",
    "\n",
    "You can now proceed to the next notebook in the series.\n",
    "</div>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "id": "de90629c-0f8d-44b4-8658-3d4dec898887",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bc69b39-4684-43ef-b881-197e1cf8a3c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# response = oss_client.indices.delete(index=INDEX_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2e6a7fe-aacd-4f7b-ae32-87cf68a2e986",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
