{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1",
   "metadata": {},
   "source": [
    "# Step 3: Video Generation with Text and Image\n",
    "\n",
    "## Image-Conditioned Video Generation with AWS Bedrock\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to this comprehensive notebook on Image-Conditioned Video Generation using AWS Bedrock's `nova-reel` model. This notebook demonstrates how to generate videos that are influenced by both textual descriptions and reference images, providing greater control over the visual style and content of the generated output.\n",
    "\n",
    "Image-conditioned video generation represents an advanced application of generative AI, combining the capabilities of text-to-video and image-to-video generation. By providing both a text prompt and a reference image, we can guide the model to create videos that maintain visual consistency with the reference image while incorporating the actions or transformations described in the text.\n",
    "\n",
    "This approach offers several advantages over text-only video generation:\n",
    "\n",
    "* **Visual Consistency**: The generated video maintains the visual style, colors, and composition of the reference image\n",
    "* **Subject Preservation**: Specific subjects or objects from the reference image can be preserved in the video\n",
    "* **Style Control**: The aesthetic qualities of the reference image influence the generated video\n",
    "* **Scene Extension**: The video can extend or animate a static scene captured in the reference image\n",
    "\n",
    "In this notebook, we'll build upon the text-to-video generation capabilities explored in the previous notebook, adding the ability to condition the generation process with a reference image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#Setup-and-Dependencies)\n",
    "2. [Configuration and Initialization](#Configuration-and-Initialization)\n",
    "3. [Preparing the Reference Image](#Preparing-the-Reference-Image)\n",
    "4. [Building the Model Input](#Building-the-Model-Input)\n",
    "5. [Generating the Video](#Generating-the-Video)\n",
    "6. [Results and Next Steps](#Results-and-Next-Steps)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, we'll import the necessary libraries and modules for image-conditioned video generation. We'll also import functions from our previous notebooks to avoid code duplication."
   ]
  },
  {
   "cell_type": "code",
   "id": "1",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "from pathlib import Path\n",
    "import base64\n",
    "from IPython.display import Video, Image\n",
    "import botocore.exceptions\n",
    "import nbimporter\n",
    "from _00_image_processing import resize_and_encode\n",
    "from _02_video_gen_text_only import monitor_video_generation, download_and_display_video, start_video_generation, monitor_video_generation, \\\n",
    "monitor_video_generation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4",
   "metadata": {},
   "source": [
    "## Configuration and Initialization\n",
    "\n",
    "Next, we'll retrieve our stored variables from previous notebooks and initialize our AWS clients:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {},
   "source": [
    "%store -r OUTPUT_DIR\n",
    "%store -r BUCKET"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5",
   "metadata": {},
   "source": [
    "Let's define our text prompt and the path to our reference image:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3",
   "metadata": {},
   "source": [
    "PROMPT = \"camera tilt up from the road to the sky\"\n",
    "INPUT_IMAGE_PATH = \"images/road.jpg\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6",
   "metadata": {},
   "source": [
    "Now we'll initialize the AWS clients we'll need for video generation:"
   ]
  },
  {
   "cell_type": "code",
   "id": "4",
   "metadata": {},
   "source": [
    "# Initialize AWS clients\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "execution_role = sagemaker.get_execution_role()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5",
   "metadata": {},
   "source": [
    "print(f\"Execution Role: {execution_role}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7",
   "metadata": {},
   "source": [
    "## Preparing the Reference Image\n",
    "\n",
    "### Displaying the Reference Image\n",
    "\n",
    "First, let's display the reference image to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6",
   "metadata": {},
   "source": [
    "Image(INPUT_IMAGE_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8",
   "metadata": {},
   "source": [
    "### Encoding the Reference Image\n",
    "\n",
    "Next, we need to encode the image to Base64 format for the Bedrock API. We'll use the `resize_and_encode` function from our image processing notebook to ensure the image is properly formatted:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> The image is resized to 1280x720 resolution to match the output video dimensions, ensuring visual consistency between the reference image and the generated video.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7",
   "metadata": {},
   "source": [
    "# Encode input image\n",
    "input_image_base64 = resize_and_encode(INPUT_IMAGE_PATH, output_size=(1280, 720))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9",
   "metadata": {},
   "source": [
    "## Building the Model Input\n",
    "\n",
    "### Function: `build_model_input`\n",
    "\n",
    "This function constructs the payload for the Bedrock API, incorporating both the text prompt and the reference image. It's similar to the function in the previous notebook but with modifications to handle the image input.\n",
    "\n",
    "#### Key Parameters\n",
    "\n",
    "- `prompt`: The **text prompt** describing the desired scene or action.\n",
    "- `image_base64`: The **reference image** in Base64 format.\n",
    "- `duration`: The **length** of the video in seconds.\n",
    "- `fps`: **Frame rate** of the video.\n",
    "- `resolution`: **Output video resolution** (default: `1280x720`).\n",
    "- `seed`: A **random seed** for consistent generation.\n",
    "\n",
    "#### Functionality\n",
    "\n",
    "- Constructs the `textToVideoParams` object with both text and image inputs.\n",
    "- Defines video generation parameters such as duration, FPS, and resolution.\n",
    "- Returns a complete payload ready for submission to the Bedrock API."
   ]
  },
  {
   "cell_type": "code",
   "id": "8",
   "metadata": {},
   "source": [
    "def build_model_input(prompt, image_base64=None, duration=6, fps=24, resolution=\"1280x720\", seed=0):\n",
    "    \"\"\"\n",
    "    Constructs the input payload for the Bedrock video generation model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Text prompt for video generation.\n",
    "        image_base64 (str): Base64 encoded image data.\n",
    "        duration (int): Duration of the video in seconds.\n",
    "        fps (int): Frames per second for the video.\n",
    "        resolution (str): Resolution of the video.\n",
    "        seed (int): Seed for deterministic generation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The formatted model input payload.\n",
    "    \"\"\"\n",
    "    if image_base64:\n",
    "        image_parameter = [{\"format\": \"png\", \"source\": {\"bytes\": image_base64}}]\n",
    "    else:\n",
    "        image_parameter = []\n",
    "    return {\n",
    "        \"taskType\": \"TEXT_VIDEO\",\n",
    "        \"textToVideoParams\": {\n",
    "            \"text\": prompt,\n",
    "            \"images\": image_parameter\n",
    "        },\n",
    "        \"videoGenerationConfig\": {\n",
    "            \"durationSeconds\": duration,\n",
    "            \"fps\": fps,\n",
    "            \"dimension\": resolution,\n",
    "            \"seed\": seed\n",
    "        },\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c10",
   "metadata": {},
   "source": [
    "Now we'll build the model input payload using our prompt and reference image:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9",
   "metadata": {},
   "source": [
    "# Build the model input payload\n",
    "model_input = build_model_input(PROMPT, input_image_base64)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c11",
   "metadata": {},
   "source": [
    "## Generating the Video\n",
    "\n",
    "Now we'll use the functions imported from the previous notebook to generate our video. The process involves three main steps:\n",
    "\n",
    "1. **Starting the Video Generation Job**: Submitting our payload to the Bedrock API\n",
    "2. **Monitoring the Job Status**: Waiting for the video generation to complete\n",
    "3. **Downloading and Displaying the Result**: Retrieving the video from S3 and displaying it in the notebook\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Important:</b> The following cells will start the video generation process, which may take several minutes to complete. Make sure you're using the conda_python3 environment to avoid any issues with the Bedrock API.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "10",
   "metadata": {},
   "source": [
    "# Start video generation / If start_async_invoke fails here make sure conda_python3 env is selected in the notebook.\n",
    "invocation = start_video_generation(bedrock_runtime, BUCKET, model_input)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c12",
   "metadata": {},
   "source": [
    "Now we'll monitor the job status until it completes:"
   ]
  },
  {
   "cell_type": "code",
   "id": "11",
   "metadata": {},
   "source": [
    "# Track job status\n",
    "video_uri = monitor_video_generation(bedrock_runtime, invocation[\"invocationArn\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c13",
   "metadata": {},
   "source": [
    "Finally, we'll download and display the generated video:"
   ]
  },
  {
   "cell_type": "code",
   "id": "12",
   "metadata": {},
   "source": [
    "# Download and display the video\n",
    "download_and_display_video(s3_client, BUCKET, video_uri, OUTPUT_DIR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c14",
   "metadata": {},
   "source": [
    "## Results and Next Steps\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>🎉 Congratulations!</b> You have successfully completed the image-conditioned video generation notebook!\n",
    "\n",
    "Key accomplishments:\n",
    "- ✅ Prepared a reference image for video generation\n",
    "- ✅ Combined text and image inputs in the model payload\n",
    "- ✅ Generated a video that maintains visual consistency with the reference image\n",
    "- ✅ Downloaded and displayed the generated video\n",
    "</div>\n",
    "\n",
    "### Comparing Text-Only vs. Image-Conditioned Generation\n",
    "\n",
    "The key difference between this notebook and the previous one is the addition of a reference image to guide the video generation process. This provides several advantages:\n",
    "\n",
    "1. **Visual Consistency**: The generated video maintains the visual style and elements of the reference image\n",
    "2. **Greater Control**: The combination of text and image inputs allows for more precise control over the generated content\n",
    "3. **Subject Preservation**: Specific subjects or objects from the reference image are preserved in the video\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebooks, we'll explore more advanced video generation techniques, including:\n",
    "\n",
    "- Multi-step video generation for more complex sequences\n",
    "- Inpainting techniques for modifying specific regions of videos\n",
    "- Combining multiple techniques for advanced video creation\n",
    "\n",
    "Proceed to the next notebook to continue exploring AWS Bedrock's video generation capabilities."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "83aba7719a1993d1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
